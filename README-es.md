<h1 align="center">
    <a href="https://crawlee.dev">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/apify/crawlee-python/master/website/static/img/crawlee-dark.svg?sanitize=true">
          <img alt="Crawlee" src="https://raw.githubusercontent.com/apify/crawlee-python/master/website/static/img/crawlee-light.svg?sanitize=true" width="500">
        </picture>
    </a>
    <br>
    <small>Una biblioteca de web scraping y automatizaci√≥n de navegadores</small>
</h1>

<p align=center>
    <a href="https://trendshift.io/repositories/11169" target="_blank"><img src="https://trendshift.io/api/badge/repositories/11169" alt="apify%2Fcrawlee-python | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</p>

<p align=center>
    <a href="https://badge.fury.io/py/crawlee" rel="nofollow">
        <img src="https://badge.fury.io/py/crawlee.svg" alt="Versi√≥n de PyPI" style="max-width: 100%;">
    </a>
    <a href="https://pypi.org/project/crawlee/" rel="nofollow">
        <img src="https://img.shields.io/pypi/dm/crawlee" alt="PyPI - Descargas" style="max-width: 100%;">
    </a>
    <a href="https://pypi.org/project/crawlee/" rel="nofollow">
        <img src="https://img.shields.io/pypi/pyversions/crawlee" alt="PyPI - Versi√≥n de Python" style="max-width: 100%;">
    </a>
    <a href="https://discord.gg/jyEM2PRvMU" rel="nofollow">
        <img src="https://img.shields.io/discord/801163717915574323?label=discord" alt="Chatea en Discord" style="max-width: 100%;">
    </a>
</p>

Crawlee cubre tu rastreo y scraping de principio a fin y **te ayuda a construir scrapers fiables. R√°pido.**

Tus crawlers parecer√°n casi humanos y pasar√°n desapercibidos para las protecciones de bots modernas incluso con la configuraci√≥n predeterminada. Crawlee te da las herramientas para rastrear la web en busca de enlaces, extraer datos y almacenarlos de forma persistente en formatos legibles por m√°quina, sin tener que preocuparte por los detalles t√©cnicos. Y gracias a las ricas opciones de configuraci√≥n, puedes ajustar casi cualquier aspecto de Crawlee para adaptarlo a las necesidades de tu proyecto si la configuraci√≥n predeterminada no es suficiente.

> üëâ **Consulta la documentaci√≥n completa, gu√≠as y ejemplos en el [sitio web del proyecto Crawlee](https://crawlee.dev/python/)** üëà

Tambi√©n tenemos una implementaci√≥n de Crawlee en TypeScript, que puedes explorar y utilizar para tus proyectos. Visita nuestro repositorio de GitHub para m√°s informaci√≥n [Crawlee para JS/TS en GitHub](https://github.com/apify/crawlee).

## Instalaci√≥n

Te recomendamos visitar el [tutorial de Introducci√≥n](https://crawlee.dev/python/docs/introduction) en la documentaci√≥n de Crawlee para m√°s informaci√≥n.

Crawlee est√° disponible como el paquete [`crawlee`](https://pypi.org/project/crawlee/) en PyPI. Este paquete incluye la funcionalidad principal, mientras que las caracter√≠sticas adicionales est√°n disponibles como extras opcionales para mantener las dependencias y el tama√±o del paquete al m√≠nimo.

Para instalar Crawlee with all features, ejecuta el siguiente comando:

```sh
python -m pip install 'crawlee[all]'
```

Luego, instala las dependencias de [Playwright](https://playwright.dev/):

```sh
playwright install
```

Verifica que Crawlee se ha instalado correctamente:

```sh
python -c 'import crawlee; print(crawlee.__version__)'
```

Para instrucciones de instalaci√≥n detalladas, consulta la p√°gina de documentaci√≥n [Configuraci√≥n](https://crawlee.dev/python/docs/introduction/setting-up).

### Con Crawlee CLI

La forma m√°s r√°pida de empezar con Crawlee es usando la CLI de Crawlee y seleccionando una de las plantillas preparadas. Primero, aseg√∫rate de tener [uv](https://pypi.org/project/uv/) instalado:

```sh
uv --help
```

Si [uv](https://pypi.org/project/uv/) no est√° instalado, sigue la [gu√≠a de instalaci√≥n oficial](https://docs.astral.sh/uv/getting-started/installation/).

Luego, ejecuta la CLI y elige entre las plantillas disponibles:

```sh
uvx 'crawlee[cli]' create my-crawler
```

Si ya tienes `crawlee` instalado, puedes iniciarlo ejecutando:

```sh
crawlee create my-crawler
```

## Ejemplos

Aqu√≠ tienes algunos ejemplos pr√°cticos para ayudarte a empezar con diferentes tipos de crawlers en Crawlee. Cada ejemplo demuestra c√≥mo configurar y ejecutar un crawler para casos de uso espec√≠ficos, ya sea que necesites manejar p√°ginas HTML simples o interactuar con sitios con mucho JavaScript. La ejecuci√≥n de un crawler crear√° un directorio `storage/` en tu directorio de trabajo actual.

### BeautifulSoupCrawler

El [`BeautifulSoupCrawler`](https://crawlee.dev/python/api/class/BeautifulSoupCrawler) descarga p√°ginas web usando una biblioteca HTTP y proporciona contenido analizado en HTML al usuario. Por defecto, utiliza [`HttpxHttpClient`](https://crawlee.dev/python/api/class/HttpxHttpClient) para la comunicaci√≥n HTTP y [BeautifulSoup](https://pypi.org/project/beautifulsoup4/) para analizar HTML. Es ideal para proyectos que requieren una extracci√≥n eficiente de datos de contenido HTML. Este crawler tiene un rendimiento muy bueno ya que no utiliza un navegador. Sin embargo, si necesitas ejecutar JavaScript del lado del cliente para obtener tu contenido, esto no ser√° suficiente y necesitar√°s usar [`PlaywrightCrawler`](https://crawlee.dev/python/api/class/PlaywrightCrawler). Adem√°s, si quieres usar este crawler, aseg√∫rate de instalar `crawlee` con el extra `beautifulsoup`.

```python
import asyncio

from crawlee.crawlers import BeautifulSoupCrawler, BeautifulSoupCrawlingContext


async def main() -> None:
    crawler = BeautifulSoupCrawler(
        # Limita el rastreo a un m√°ximo de solicitudes. Elim√≠nalo o aum√©ntalo para rastrear todos los enlaces.
        max_requests_per_crawl=10,
    )

    # Define el manejador de solicitudes predeterminado, que se llamar√° para cada solicitud.
    @crawler.router.default_handler
    async def request_handler(context: BeautifulSoupCrawlingContext) -> None:
        context.log.info(f'Procesando {context.request.url} ...')

        # Extrae datos de la p√°gina.
        data = {
            'url': context.request.url,
            'title': context.soup.title.string if context.soup.title else None,
        }

        # Env√≠a los datos extra√≠dos al conjunto de datos predeterminado.
        await context.push_data(data)

        # Encola todos los enlaces encontrados en la p√°gina.
        await context.enqueue_links()

    # Ejecuta el crawler con la lista inicial de URLs.
    await crawler.run(['https://crawlee.dev'])


if __name__ == '__main__':
    asyncio.run(main())
```

### PlaywrightCrawler

El [`PlaywrightCrawler`](https://crawlee.dev/python/api/class/PlaywrightCrawler) utiliza un navegador sin cabeza para descargar p√°ginas web y proporciona una API para la extracci√≥n de datos. Est√° construido sobre [Playwright](https://playwright.dev/), una biblioteca de automatizaci√≥n dise√±ada para gestionar navegadores sin cabeza. Sobresale en la recuperaci√≥n de p√°ginas web que dependen de JavaScript del lado del cliente para la generaci√≥n de contenido, o tareas que requieren interacci√≥n con contenido impulsado por JavaScript. Para escenarios donde la ejecuci√≥n de JavaScript es innecesaria o se requiere un mayor rendimiento, considera usar el [`BeautifulSoupCrawler`](https://crawlee.dev/python/api/class/BeautifulSoupCrawler). Adem√°s, si quieres usar este crawler, aseg√∫rate de instalar `crawlee` con el extra `playwright`.

```python
import asyncio

from crawlee.crawlers import PlaywrightCrawler, PlaywrightCrawlingContext


async def main() -> None:
    crawler = PlaywrightCrawler(
        # Limita el rastreo a un m√°ximo de solicitudes. Elim√≠nalo o aum√©ntalo para rastrear todos los enlaces.
        max_requests_per_crawl=10,
    )

    # Define el manejador de solicitudes predeterminado, que se llamar√° para cada solicitud.
    @crawler.router.default_handler
    async def request_handler(context: PlaywrightCrawlingContext) -> None:
        context.log.info(f'Procesando {context.request.url} ...')

        # Extrae datos de la p√°gina.
        data = {
            'url': context.request.url,
            'title': await context.page.title(),
        }

        # Env√≠a los datos extra√≠dos al conjunto de datos predeterminado.
        await context.push_data(data)

        # Encola todos los enlaces encontrados en la p√°gina.
        await context.enqueue_links()

    # Ejecuta el crawler con la lista inicial de solicitudes.
    await crawler.run(['https://crawlee.dev'])


if __name__ == '__main__':
    asyncio.run(main())
```

### M√°s ejemplos

Explora nuestra p√°gina de [Ejemplos](https://crawlee.dev/python/docs/examples) en la documentaci√≥n de Crawlee para una amplia gama de casos de uso y demostraciones adicionales.

## Caracter√≠sticas

¬øPor qu√© Crawlee es la opci√≥n preferida para el web scraping y el rastreo?

### ¬øPor qu√© usar Crawlee en lugar de una biblioteca HTTP cualquiera con un analizador de HTML?

- Interfaz unificada para rastreo **HTTP y de navegador sin cabeza**.
- **Rastreo paralelo** autom√°tico basado en los recursos del sistema disponibles.
- Escrito en Python con **sugerencias de tipo** - mejora la DX (autocompletado del IDE) y reduce los errores (comprobaci√≥n de tipo est√°tica).
- **Reintentos autom√°ticos** en caso de errores o cuando te bloquean.
- **Rotaci√≥n de proxy** y gesti√≥n de sesiones integradas.
- **Enrutamiento de solicitudes** configurable - dirige las URLs a los manejadores apropiados.
- **Cola persistente** para las URLs a rastrear.
- **Almacenamiento** conectable tanto de datos tabulares como de archivos.
- **Manejo de errores** robusto.

### ¬øPor qu√© usar Crawlee en lugar de Scrapy?

- **Basado en Asyncio** ‚Äì Aprovechando la biblioteca est√°ndar [Asyncio](https://docs.python.org/3/library/asyncio.html), Crawlee ofrece un mejor rendimiento y una compatibilidad perfecta con otras bibliotecas as√≠ncronas modernas.
- **Sugerencias de tipo** ‚Äì Proyecto m√°s nuevo construido con Python moderno y cobertura completa de sugerencias de tipo para una mejor experiencia de desarrollador.
- **Integraci√≥n simple** ‚Äì Los crawlers de Crawlee son scripts de Python regulares, que no requieren ning√∫n ejecutor de lanzador adicional. Esta flexibilidad permite integrar un crawler directamente en otras aplicaciones.
- **Persistencia de estado** ‚Äì Admite la persistencia del estado durante las interrupciones, ahorrando tiempo y costos al evitar la necesidad de reiniciar las canalizaciones de scraping desde cero despu√©s de un problema.
- **Almacenamiento de datos organizado** ‚Äì Permite guardar m√∫ltiples tipos de resultados en una sola ejecuci√≥n de scraping. Ofrece varias opciones de almacenamiento (ver [conjuntos de datos](https://crawlee.dev/python/api/class/Dataset) y [almacenes de clave-valor](https://crawlee.dev/python/api/class/KeyValueStore)).

## Ejecuci√≥n en la plataforma Apify

Crawlee es de c√≥digo abierto y se ejecuta en cualquier lugar, but since it's developed by [Apify](https://apify.com), it's easy to set up on the Apify platform and run in the cloud. Visit the [Apify SDK website](https://docs.apify.com/sdk/python/) to learn more about deploying Crawlee to the Apify platform.

## Soporte

Si encuentras alg√∫n error o problema con Crawlee, por favor [env√≠a un issue en GitHub](https://github.com/apify/crawlee-python/issues). Para preguntas, puedes preguntar en [Stack Overflow](https://stackoverflow.com/questions/tagged/apify), en las Discusiones de GitHub o puedes unirte a nuestro [servidor de Discord](https://discord.com/invite/jyEM2PRvMU).

## Contribuciones

Tus contribuciones de c√≥digo son bienvenidas, ¬°y ser√°s elogiado por la eternidad! Si tienes alguna idea de mejora, env√≠a un issue o crea una pull request. Para las directrices de contribuci√≥n y el c√≥digo de conducta, consulta [CONTRIBUTING.md](https://github.com/apify/crawlee-python/blob/master/CONTRIBUTING.md).

## Licencia

Este proyecto est√° licenciado bajo la Licencia Apache 2.0 - consulta el archivo [LICENSE](https://github.com/apify/crawlee-python/blob/master/LICENSE) para m√°s detalles.
